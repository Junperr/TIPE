{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Junperr/TIPE/blob/main/AI/all_station.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZYplsvFVcQ1"
      },
      "source": [
        "# Importation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici on télécharge les modules pour la cartographie"
      ],
      "metadata": {
        "id": "FprBohlDW5mm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "augJP5M6-ma_"
      },
      "outputs": [],
      "source": [
        "!pip install geopandas\n",
        "!pip install pandas  #geopanda dependencies\n",
        "!pip install fiona   #geopanda dependencies\n",
        "!pip install shapely #geopanda dependencies\n",
        "!pip install pyproj  #geopanda dependencies\n",
        "!pip install rtree   #geopanda dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On importe tout les modules que l'ont utilise par la suite et on connecte l'environnement au drive"
      ],
      "metadata": {
        "id": "Iej10YI-XEVF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1LiC8yB70QV"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from six.moves import urllib\n",
        "import sqlite3 as sql\n",
        "import tensorflow.compat.v2.feature_column as fc\n",
        "\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On donne l'acces à google collab à certain fichiers du drive"
      ],
      "metadata": {
        "id": "rYJbI6mfYjmQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJnmHxhE308n"
      },
      "outputs": [],
      "source": [
        "!ls /content/drive/MyDrive/Info/Tipe/sismic1.db\n",
        "!ls /content/drive/MyDrive/Info/Tipe/Shapefiles/polbnda_jpn.shp\n",
        "!ls /content/drive/MyDrive/Info/Tipe/Shapefiles/coastl_jpn.shp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_D7zwDoVlwi"
      },
      "source": [
        "# Parcours des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl8BX6qoHaxF"
      },
      "source": [
        "Ci dessous l'on définit nos set de données la partie pour entraîner le modèle et celle pour vérifier que l'on peut généraliser notre modèle (on verifie sa précision sur des données qu'il n'a jamais vu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFA0gWW274QE"
      },
      "outputs": [],
      "source": [
        "con=sql.connect('/content/drive/MyDrive/Info/Tipe/sismic1.db')\n",
        "table_earth = pd.read_sql(\"\"\"SELECT * FROM seismes order by mag desc\"\"\",con)\n",
        "table_sta = pd.read_sql(\"\"\"SELECT * FROM station \"\"\",con)\n",
        "table_info = pd.read_sql(\"\"\"SELECT * FROM infos \"\"\",con)\n",
        "print('infos :',table_info.head(),'earthquake :',table_earth.head(),\"station :\",table_sta.head(),sep='\\n'+'####'+'\\n')\n",
        "# station_list = pd.read_sql(\"\"\"SELECT PGA,PGV,mean,std,PGA_UD,PGA_EW,PGA_NS FROM infos \"\"\",sql.connect('sismic1.db'))\n",
        "\n",
        "dftrain = pd.read_sql(\"\"\"\n",
        "SELECT st.code,earthquake_id,mag,PGA,PGV,mean,std,PGA_UD,PGA_EW,PGA_NS FROM infos join station as st\n",
        "on st.code = infos.station_code join seismes as se\n",
        "on se.id==infos.earthquake_id\n",
        "Group By earthquake_id\n",
        "Order by mag DESC \"\"\",sql.connect('/content/drive/MyDrive/Info/Tipe/sismic1.db'))\n",
        "print('dftrain',dftrain.head(),sep='\\n')\n",
        "# print(dftrain,dftrain.loc[0], sep='\\n')\n",
        "print(len(dftrain))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp6PDDnSgK1a"
      },
      "source": [
        "Quelque graphes pour visualiser la distributions des données:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wv6-luKU8Bb"
      },
      "outputs": [],
      "source": [
        "dfstat_seisme = pd.read_sql(\"\"\"\n",
        "SELECT AVG(nbr_record) as nbr_moy_record,count(*) as nbr_earth,mag from (\n",
        "SELECT count(*) as nbr_record,se.id,se.mag FROM infos join  seismes as se\n",
        "on se.id==infos.earthquake_id\n",
        "group by se.id\n",
        "--having nbr_record >20\n",
        ")\n",
        "group by mag\n",
        "order by mag desc\"\"\",sql.connect('/content/drive/MyDrive/Info/Tipe/sismic1.db'))\n",
        "#dftrain['nbr_record']\n",
        "ax = dfstat_seisme.plot.scatter(x='mag', y='nbr_moy_record', c='nbr_earth',grid=True,use_index=True,figsize=(15,7),title='test',xlabel=\"magnitude\",cmap='winter',colorbar=True)\n",
        "ax.set_xticks([0.5*x for x in range(4,17)])\n",
        "ax.set_xticklabels([0.5*x for x in range(4,17)])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOOb23Lm_JMs"
      },
      "outputs": [],
      "source": [
        "dftrain.mag.hist(bins=80).set_xlabel(\"magnitude\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YlwNfBfOhR1"
      },
      "source": [
        "# Data Normalisation and selection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji-s1WPFG6-k"
      },
      "source": [
        "Ici l'on a un problème contrairement au premier modèle ou l'input était de shape (3,) (mag,long,lat) et l'output de shape (1,) le PGA de la station pour lequel le modèle était entraîné.\n",
        "Ici l'on veut le PGA (ou PGV cela ne change rien dans les faits) à toute les stations, cependant un séisme n'est pas toujours détecté par les mêmes stations et donc pas le même nombre de station il faut donc modifier nos données pour que pour tout couple (earthquake,station) on ait une valeur pour le PGA, pour l'entrainement du modèle comme la vérification a posteriori, on va donc la compléter par des 0 (avant de normer) quand on n'a pas de donnée."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ci-dessous on donne un nouvel identifiant"
      ],
      "metadata": {
        "id": "R6PI_Z78fNFU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXTJOlG8OnZQ"
      },
      "outputs": [],
      "source": [
        "order_sta5col = pd.read_sql(\"\"\" SELECT Count(*) as nb_seismes,station_code,lat,long,network FROM infos Join station\n",
        "                            On infos.station_code = station.code\n",
        "                            Group By station_code\n",
        "                            Order By nb_seismes DESC \"\"\",con)\n",
        "order_sta=order_sta5col['station_code'].tolist()\n",
        "data_set = pd.read_sql(\"\"\" SELECT id,mag,long,lat FROM seismes \"\"\",con)\n",
        "#print(data_set)\n",
        "#code_to_int=[]\n",
        "#for x in range(len(data_set)):\n",
        "#  code_to_int.append( order_sta.index(data_set['station_code'][x]))\n",
        "#data_set.pop('station_code')\n",
        "#data_set['station_code']=code_to_int\n",
        "\n",
        "data_set_normed= data_set.copy()\n",
        "for x in data_set.keys()[1:]:#aucun interet de normaliser l'id elle nous sert pour associer les valeurs de PGA après\n",
        "  data_set_normed[x] = (data_set[x]-data_set[x].mean())/data_set[x].std()\n",
        "print(data_set,data_set_normed,sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCV-p8NCIXZX"
      },
      "outputs": [],
      "source": [
        "nq=len(data_set) # number of earthquake in the data base\n",
        "ns=len(order_sta) # number of station in the data base\n",
        "tab=np.zeros([nq,ns])\n",
        "# un tableau qui pour l'indice [i,j] donne le PGA enregistrer par la station étant\n",
        "# à l'indice j dans order_sta durant le seisme d'id i+1 (les indices commence à 0)\n",
        "# alors que les id à 1\n",
        "\n",
        "for i in range (0,nq):\n",
        "    station_detected = pd.read_sql(\"\"\"Select PGA,station_code From infos\n",
        "    Where earthquake_id = {}\n",
        "    \"\"\".format(i+1),con)\n",
        "    # pour chaque séismes on récupère les stations qui l'ont enregistré\n",
        "    for j in station_detected['station_code']:\n",
        "      tab[i,order_sta.index(j)] = pd.read_sql(\"\"\"Select PGA From infos\n",
        "    Where earthquake_id = {} and station_code = '{}'\n",
        "    \"\"\".format(i+1,j),con)['PGA'][0]\n",
        "    # et on met à jour le tableau avec la valeur de PGA de sismic1\n",
        "print(tab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjIz5xLEQ0xV"
      },
      "outputs": [],
      "source": [
        "# cette cellule permet juste de s'assurer que notre programme marche bien\n",
        "count=[0 for x in range (ns)]\n",
        "for i in range (nq):\n",
        "  for j in range (ns):\n",
        "    if tab[i,j]!=0:\n",
        "      count[j]+=1\n",
        "print(count)\n",
        "print(order_sta5col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxoW3nqL8QeH"
      },
      "outputs": [],
      "source": [
        "print(tab)\n",
        "way_back=[tab.mean(),tab.std()]\n",
        "tab = (tab - tab.mean())/tab.std()\n",
        "print(tab,tab.mean(),tab.std(),sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nsj6Uph5BhBG"
      },
      "outputs": [],
      "source": [
        "data_set_normed = data_set_normed.sample(frac=1).reset_index(drop=True)\n",
        "# shuffle our data_set, drop=True mean we do not keep old index on a new column\n",
        "n=len(data_set_normed)\n",
        "data_train = data_set_normed[:int(0.90*n)]\n",
        "data_eval = data_set_normed[int(0.90*n):]\n",
        "data_eval = data_eval.reset_index(drop=True)\n",
        "print(data_train)\n",
        "train_id = data_train.pop('id').tolist()\n",
        "eval_id = data_eval.pop('id').tolist()\n",
        "print(eval_id,train_id,sep='\\n')\n",
        "result_train=np.zeros([len(train_id),ns])\n",
        "result_eval=np.zeros([len(eval_id),ns])\n",
        "print(len(train_id))\n",
        "for i in range(len(train_id)):\n",
        "  result_train[i] = tab[train_id[i]-1]\n",
        "for i in range(len(eval_id)):\n",
        "  result_eval[i] = tab[eval_id[i]-1]\n",
        "print(result_train,result_eval,sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3Dcx9XQ09FX"
      },
      "source": [
        "On voit que la contruction du tableau correspond bien à ce que l'on voulait.\n",
        "\n",
        "Maintenant pour chaque seismes on a une sortie de shape (ns,) on a donc plus le problème précédent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0FuiEHHVt3C"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsl5gnd_H1P8"
      },
      "source": [
        "On créer notre model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8t0zSbjHWsC"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(2000, activation=tf.nn.elu,input_shape=[3]),\n",
        "  tf.keras.layers.Dense(5000, activation=tf.nn.elu),\n",
        "  tf.keras.layers.Dense(units=ns, input_shape=[ns])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfqmMMAdI-Mm"
      },
      "source": [
        "On le compile:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPqlIagSJBLv"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"mae\",\n",
        "              optimizer=tf.keras.optimizers.Adam(0.001),metrics=['mae','mse','accuracy'],run_eagerly=True)\n",
        "list_optimizers = ['SGD', 'RMSprop', 'Adam', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']\n",
        "# on peut en essayer des différents (pour l'instant Adam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id58zxVVmqu_"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ML3g70Va3zME"
      },
      "outputs": [],
      "source": [
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
        "\n",
        "history = model.fit(data_train, result_train, epochs=500,validation_split=0.2, verbose=False, callbacks=[early_stop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OM3spXCoeiEC"
      },
      "outputs": [],
      "source": [
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "hist.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1QdeThWexti"
      },
      "outputs": [],
      "source": [
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  #plt.ylim([0, 100])\n",
        "  #plt.xlim([0,50])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error [PGA]')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "plot_loss(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeSRb0FeMrHk"
      },
      "source": [
        "On cherche ici à trouver un \"bon\" séisme dans le set de donnée que le model n'a encore jamais vus pour ensuite comparer les carte réelle et celle predit par le model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hIjOjZMMbcF"
      },
      "outputs": [],
      "source": [
        "order_quake2col = pd.read_sql(\"\"\"\n",
        "Select Count(*) As nbr_sta,earthquake_id From infos\n",
        "Group By earthquake_id\n",
        "Order By nbr_sta Desc\n",
        "\"\"\",con)\n",
        "order_quake = order_quake2col['earthquake_id'].tolist()\n",
        "print(order_quake[0],order_quake2col['earthquake_id'][0])\n",
        "\n",
        "trick_sort = [order_quake.index(x) for x in eval_id]\n",
        "print(order_quake)\n",
        "print(data_eval)\n",
        "data_eval['id']=trick_sort\n",
        "print(data_eval)\n",
        "data_eval.sort_values('id',axis=0,ascending=True,inplace=True)\n",
        "print(data_eval)\n",
        "sorted_id = data_eval.pop('id')\n",
        "real_id_sorted = [order_quake[x] for x in sorted_id]\n",
        "data_eval['id'] = real_id_sorted\n",
        "print(data_eval)\n",
        "print(data_eval[15:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUyOR84H0_O_"
      },
      "outputs": [],
      "source": [
        "guineapig = data_eval[0:1]\n",
        "print(guineapig,way_back)\n",
        "id_guineapig = guineapig.pop('id')\n",
        "true_values = tab[id_guineapig]\n",
        "result_to_plot = model.predict(guineapig)\n",
        "true_values = (true_values * way_back[1]) + way_back[0]\n",
        "result_to_plot = (result_to_plot * way_back[1]) + way_back[0]\n",
        "print(true_values,result_to_plot,result_to_plot.shape,sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CArapux2S0p7"
      },
      "source": [
        "# Cartographie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZISXIbFIyEm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as colors\n",
        "import shapely.geometry as geo\n",
        "from scipy.interpolate import griddata\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "import rtree\n",
        "con=sql.connect('/content/drive/MyDrive/Info/Tipe/sismic1.db')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7O33-KuI3Gk"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 2,figsize=(32,9))\n",
        "X = np.linspace(0,np.pi,150)\n",
        "def plot_f(f):\n",
        "  plt.plot(X,f(X))\n",
        "plt.subplot(1,2,1)\n",
        "plot_f(np.cos)\n",
        "plt.subplot(1,2,2)\n",
        "plot_f(np.sin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqQ1WzktJzMO"
      },
      "source": [
        "Les variables (globales) nécessaire à map_plot\n",
        "(elle ne sont pas définie de façon locale pour éviter de les définir à nouveaux à chaque appel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BzIfmxIJwmR"
      },
      "outputs": [],
      "source": [
        "japan_f = gpd.read_file(\"/content/drive/MyDrive/Info/Tipe/Shapefiles/polbnda_jpn.shp\")\n",
        "japan_c = gpd.read_file(\"/content/drive/MyDrive/Info/Tipe/Shapefiles/coastl_jpn.shp\")\n",
        "# open two different japan map shape one with area one without\n",
        "\n",
        "\n",
        "# read sql database for station and create panda dataframe\n",
        "crs='EPSG:8994'\n",
        "# système de coordonnées\n",
        "\n",
        "\n",
        "# accel_colors=['accel','#ffffff','#99b3e6','#4073d9','#33ccad','#06d039','#99ff60','#ffff00','#ff9a34','#ff6275','#990000']\n",
        "accel_colors=['accel',[255,255,255],[150,180,230],[70,130,230],[50,200,180],[30,220,80],[150,255,90],[255,255,0],[255,170,70],[255,80,120],[150,0,0]]\n",
        "accel_colors[1:] = [[x[0]/255,x[1]/255,x[2]/255] for x in accel_colors[1:]]\n",
        "print(accel_colors)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"x = np.arange(1, 10)\n",
        "y = x.reshape(-1, 1)\n",
        "h = x * y\n",
        "\n",
        "cs = plt.contourf(h, levels=[10, 30, 50],\n",
        "    colors=['#808080', '#A0A0A0', '#C0C0C0'], extend='both')\n",
        "cs.changed()\n",
        "plt.colorbar()\"\"\""
      ],
      "metadata": {
        "id": "_JnxqKwO6i2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fig,ax = plt.subplots(figsize=(40,40))\n",
        "#s_pos=pd.read_sql(\"\"\"SELECT long,lat,network,PGA FROM station as s join infos as i\n",
        "#                  on s.code=i.station_code\n",
        "#                  where i.earthquake_id=3\"\"\",con)\n",
        "# read sql database for station and create panda dataframe\n",
        "# système de coordonnées\n",
        "\n",
        "\"\"\"\n",
        "geometry = [geo.Point(s_pos[\"long\"][i],s_pos[\"lat\"][i]) for i in range (len(s_pos[\"long\"]))]\n",
        "\n",
        "geo_s_pos = gpd.GeoDataFrame(s_pos,crs=crs,geometry=geometry)\n",
        "\n",
        "s_pos_s=pd.read_sql('SELECT lat,long FROM seismes where id=3',\\\n",
        "            con)\n",
        "geometry_s = [geo.Point(s_pos_s[\"long\"][i],s_pos_s[\"lat\"][i]) for i in range (len(s_pos_s[\"long\"]))]\n",
        "geo_s_pos_s = gpd.GeoDataFrame(s_pos_s,crs=crs,geometry=geometry_s)\n",
        "\n",
        "japan_f.plot(ax=ax,color=\"w\",edgecolor=\"w\",zorder=1) # map of Japan areas\n",
        "japan_c.plot(ax=ax,color=\"k\",edgecolor=\"none\",linewidth=0.5,zorder=3) # physical border of Japan\n",
        "geo_s_pos[geo_s_pos['network'] == 'kik'].plot(column=\"PGA\", ax = ax, marker = '.',\\\n",
        "            markersize = 100 ,label = 'kik station',cmap='jet')\n",
        "\n",
        "geo_s_pos[geo_s_pos['network'] == 'knt'].plot(column=\"PGA\", ax = ax, marker = '^',\\\n",
        "        markersize = 80, label = 'knt station', cmap='jet')\n",
        "geo_s_pos_s.plot(ax=ax,marker='*',color='red',markersize=90,label='sismic event')\n",
        "plt.legend(loc=4,fontsize=40,markerscale=6)\n",
        "ax.set_ylim([min(geo_s_pos['lat'])-1,max(geo_s_pos['lat'])+1])\n",
        "ax.set_xlim([min(geo_s_pos['long'])-1,max(geo_s_pos['long'])+1])\n",
        "cmap=plt.get_cmap('jet')\n",
        "bounds = np.arange(min(geo_s_pos['PGA']),max(geo_s_pos['PGA']),cmap.N)\n",
        "norm = colors.BoundaryNorm(boundaries=bounds, ncolors=cmap.N)\n",
        "scalar = plt.cm.ScalarMappable(norm=norm,cmap=cmap)\n",
        "plt.colorbar(scalar,cmap='jet')\"\"\""
      ],
      "metadata": {
        "id": "8wdKG7ay11HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWGdyJNVgoyv"
      },
      "outputs": [],
      "source": [
        "def map_plot (quake_id=93, predict = False, drop_null = True, fig = None, ax = None, map_back=True, cmap=None, colors_name = accel_colors, centered_map=True, interpolate=False, n=100,msize=50):\n",
        "    \"\"\"\n",
        "\n",
        "    Parameters:\n",
        "        map_back : Bool, optional\n",
        "            Si True affiche la carte en fond sinon uniquement les points.\n",
        "            The default is True.\n",
        "        cmap : Colormap, optional\n",
        "            To force a colormap. The default is None.\n",
        "        colors_name : List, optional\n",
        "            [string,arg*] arg are colors. The default is accel_colors.\n",
        "        interpolate: Bool, optional\n",
        "            Si True affiche le rendu avec une interpollation réaliser avec n\n",
        "                points par axes\n",
        "\n",
        "    USE:\n",
        "        Rendu de l'accéleration maximal d'un séisme sur le japon\n",
        "    Returns\n",
        "    -------\n",
        "    None.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def unormalize (tab):\n",
        "        return (tab * way_back[1]) + way_back[0]\n",
        "\n",
        "    def recup_date (text):\n",
        "        day,hour = text.split(' ')\n",
        "        hour = hour[:-3]\n",
        "        return day,hour\n",
        "    if predict :\n",
        "        try :\n",
        "            ind_eval = eval_id.index(quake_id)\n",
        "            print(\"This earthake is in evaluation set\")\n",
        "            model_input = data_eval[ind_eval:ind_eval+1][:-1]\n",
        "        except :\n",
        "            ind_train = train_id.index(quake_id)\n",
        "            print(\"This earthake is in training set\")\n",
        "            model_input = data_train[ind_train:ind_train+1]\n",
        "        result_model = model.predict(model_input)[0]\n",
        "        # return (result_model,unormalize(result_model))\n",
        "        sta_pos = {'PGA': unormalize(result_model),'geometry':[],'long':[],'lat':[],'network':[]}\n",
        "    else :\n",
        "        sta_pos = {'PGA': unormalize(tab[quake_id-1]),'geometry':[],'long':[],'lat':[],'network':[]}\n",
        "    for i in range (ns):\n",
        "        sta_pos['geometry'].append(geo.Point((order_sta5col['long'][i],order_sta5col['lat'][i])))\n",
        "        sta_pos['long'].append(order_sta5col['long'][i])\n",
        "        sta_pos['lat'].append(order_sta5col['lat'][i])\n",
        "        sta_pos['network'].append(order_sta5col['network'][i])\n",
        "    print(sta_pos)\n",
        "    geo_sta_pos = gpd.GeoDataFrame(sta_pos,crs=crs,geometry=sta_pos['geometry'])\n",
        "    pga_null = geo_sta_pos[geo_sta_pos['PGA'] == 0]\n",
        "    print('**********',pga_null,sep='\\n')\n",
        "    geo_sta_pos.drop(pga_null.index,inplace=True)\n",
        "    geo_sta_pos.reset_index(drop=True,inplace=True)\n",
        "    center_pos = pd.read_sql(f'SELECT lat,long,date,mag,depth FROM seismes where id={quake_id}',con)\n",
        "    geometry_center = [geo.Point(center_pos[\"long\"][i],center_pos[\"lat\"][i]) for i in range (len(center_pos[\"long\"]))]\n",
        "    # en pratique i=0 je ne met pas 0 par défault si jamais je veux afficher plusieurs seismes\n",
        "    # il faudrait dans ce cas modifier la requete sql\n",
        "    center_geo = gpd.GeoDataFrame(center_pos,crs=crs,geometry=geometry_center)\n",
        "    if fig == None and ax == None :\n",
        "        fig,ax = plt.subplots(figsize=(18,18))\n",
        "    ax.set_facecolor('#d3ffff')\n",
        "    if centered_map :\n",
        "        ax.set_ylim([min(geo_sta_pos['lat'])-1,max(geo_sta_pos['lat'])+1])\n",
        "        ax.set_xlim([min(geo_sta_pos['long'])-1,max(geo_sta_pos['long'])+1])\n",
        "    if map_back:\n",
        "        japan_f.plot(ax=ax,color=\"w\",edgecolor=\"w\",zorder=1) # map of Japan areas\n",
        "        japan_c.plot(ax=ax,color=\"k\",edgecolor=\"none\",linewidth=0.5,zorder=3) # physical border of Japan\n",
        "        # affiche la carte du Japon en fond\n",
        "\n",
        "    if cmap==None and colors_name!=None:\n",
        "        try:\n",
        "            create_cmap1(colors_name[1:],colors_name[0])\n",
        "            cmap=colors_name[0]\n",
        "        except:\n",
        "            cmap=colors_name[0]\n",
        "\n",
        "    # création de la colormap si elle n'est pas déjà créée\n",
        "    cmap1=plt.get_cmap(cmap,10)\n",
        "    # on récupère uniquement 10 couleurs en réalité celles de la liste accel_colors\n",
        "    # il y a une meilleure facon de faire la colormap mais ce que j'ai fais est\n",
        "    # suffisant\n",
        "    # bounds = np.array([0.0, 0.2, 1.0, 4.0, 15.0, 50.0, 100.0, 200.0, 400.0, 750.0, 2000.0])\n",
        "    bounds = np.array([0.03, 0.25, 1.0, 3.95, 15.0, 57.0, 111.5, 218.0, 425.50, 830.0, 2500.0])\n",
        "    norm = colors.BoundaryNorm(boundaries=bounds, ncolors=10)\n",
        "    scalar = plt.cm.ScalarMappable(norm=norm,cmap=cmap1)\n",
        "    bar = plt.colorbar(scalar, fraction=0.03,ax = ax, drawedges=True, spacing='uniform', ticks=[0.2,0.5,1.0,2.0,5.0,10.0,20.0,50.0,100.0,200.0,500.0,1000.0,2000.0])\n",
        "    bar.ax.tick_params(direction='inout', length=6, width=2, colors='k', grid_color='k', grid_alpha=0.5)\n",
        "    bar.set_label(label='PGA (gal)',fontsize=20)\n",
        "    bar.ax.set_yticklabels(['0.2','0.5','1.0','2.0','5.0','10.0','20.0','50.0','100.0','200.0','500.0','1000.0','2000.0'],fontsize=15)\n",
        "# met la barre à l'échelle et créer la norme et ajoute des graduations\n",
        "\n",
        "    geo_sta_pos[geo_sta_pos['network'] == 'kik'].plot(column=\"PGA\", ax = ax, marker = ',',\\\n",
        "                    markersize = 85,edgecolor=\"black\",linewidth=0.5,vmax=2000.0 ,norm=norm,label = 'KiK-NET station',cmap=cmap1,zorder=4)\n",
        "\n",
        "    geo_sta_pos[geo_sta_pos['network'] == 'knt'].plot(column=\"PGA\", ax = ax, marker = '^',\\\n",
        "             markersize=100 ,edgecolor=\"black\",linewidth=0.5,norm=norm, vmax=2000.0, label = 'K-NET station', cmap=cmap1,zorder=5)\n",
        "    # on affiche les sismogrpahes du réseaux kik et knt qui ont percu le seisme\n",
        "    center_geo.plot(ax=ax,marker='*',color='red',markersize=200,label='Epicenter',zorder=6)\n",
        "    plt.ylabel('Latitude',fontsize=20)\n",
        "    yticks = ax.get_yticks()\n",
        "    ax.set_yticklabels([str(x) for x in yticks], fontsize=15)\n",
        "    plt.xlabel('Longitude',fontsize=20)\n",
        "    xticks = ax.get_xticks()\n",
        "    ax.set_xticklabels([str(x) for x in xticks], fontsize=15)\n",
        "    ax.tick_params(axis='both', direction='out', length=6, width=2, colors='k', grid_color='k', grid_alpha=0.5)\n",
        "    ax.legend(loc=4,fontsize=24,markerscale=2.5)\n",
        "    if predict :\n",
        "        ax.set_title(label = 'Predicted Peak Acceleration Contour Map', fontsize=30,loc='center')\n",
        "    else:\n",
        "        ax.set_title(label = 'Real Peak Acceleration Contour Map', fontsize=30,loc='center')\n",
        "    day,hour = recup_date(center_pos['date'][0])\n",
        "    plt.suptitle(t = f\" {day} {hour} {str( center_pos['lat'][0] )}N {str(center_pos['long'][0])}E {str(center_pos['depth'][0])}km {str(center_pos['mag'][0])}M \", x = 0.5, y= 0.065, fontsize = 30)\n",
        "    # ajout de l'épicentre et de la legende\n",
        "\n",
        "\n",
        "    ### interpolation à partir d'ici\n",
        "    if interpolate:\n",
        "        ratio = (max(geo_sta_pos['lat'])-min(geo_sta_pos['lat']))/(max(geo_sta_pos['long'])-min(geo_sta_pos['long']))\n",
        "        if ratio <= 1:\n",
        "            xgrid,ygrid = np.mgrid[min(geo_sta_pos['long']):max(geo_sta_pos['long']):n*1j , min(geo_sta_pos['lat']):max(geo_sta_pos['lat']):ratio*n*1j]\n",
        "        else:\n",
        "            xgrid,ygrid = np.mgrid[min(geo_sta_pos['long']):max(geo_sta_pos['long']):n*1j/ratio , min(geo_sta_pos['lat']):max(geo_sta_pos['lat']):n*1j]\n",
        "        # meshgrid prend en argument le nombre de point selon un axe et non le nombre total\n",
        "        # on ajuste donc le nombre de point pour qu'ils soient répartit uniformément\n",
        "        # avec comme nombre maximum de point selon un axe n\n",
        "        point = np.array([(geo_sta_pos['long'][i],geo_sta_pos['lat'][i]) for i in range (len(geo_sta_pos['long']))])\n",
        "        # on recupere les coordonnées de tout les sismographes pour les utiliser\n",
        "        # en tant que valeurs de références pour l'interpolation\n",
        "        interpolated_values = griddata(point,np.array(geo_sta_pos['PGA']),(xgrid,ygrid),method='linear',fill_value=0.0)\n",
        "        # on créer une grille de n points sur le plus petit rectangle contenant tout les sismographes\n",
        "        # print(len(xgrid),len(xgrid[0]),len(ygrid),len(ygrid[0]),sep='\\n')\n",
        "        dataframe = {'values' : [],'geometry':[]}\n",
        "        for i in range (len(xgrid)):\n",
        "            for j in range (len(xgrid[0])):\n",
        "                dataframe['values'].append(interpolated_values[i,j])\n",
        "                dataframe['geometry'].append(geo.Point((xgrid[i,j],ygrid[i,j])))\n",
        "        dataframe['geometry']=gpd.GeoSeries(dataframe['geometry'])\n",
        "        geo_values = gpd.GeoDataFrame(dataframe,crs=crs,geometry=dataframe['geometry'])\n",
        "        geo_values1 = geo_values.overlay(japan_f,how='intersection')\n",
        "        # # print(geo_values.head())\n",
        "        geo_values1.plot(column=\"values\", ax = ax, marker = '.',\\\n",
        "                        markersize = msize, norm=norm, vmax=2000.0, label = 'knt station', cmap=cmap1,zorder=2)\n",
        "        # inprogress(ax,geo_values,map_back)\n",
        "\n",
        "    pass\n",
        "\n",
        "def create_cmap1(color_list,name='test'):\n",
        "    \"\"\"\n",
        "    PARAMETER:\n",
        "        color_list : list\n",
        "            list of colors.\n",
        "        name : str, optional\n",
        "            the name of your futur colormap. The default is 'test'.\n",
        "    USE:\n",
        "        Create a color map from a list of color in the order given in color_list\n",
        "    Return:\n",
        "        None\n",
        "    \"\"\"\n",
        "    cmap = colors.LinearSegmentedColormap.from_list(name, color_list)\n",
        "    plt.register_cmap(name, cmap)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOPaRkKdZdT2"
      },
      "outputs": [],
      "source": [
        "# print(order_sta5col.head(),tab,sep='\\n')\n",
        "# fig, axs = plt.subplots(1, 2,figsize=(40,20)) 6668\n",
        "map_plot(quake_id=3,predict=False,interpolate=True,centered_map=True,n=400)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyF72m9k_00_"
      },
      "outputs": [],
      "source": [
        "map_plot(quake_id=8288,predict=True,interpolate=True,centered_map=True,n=300)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, [ax1, ax2] = plt.subplots(nrows=1, ncols=2, figsize=(40,18))\n",
        "# plt.scatter([1,2,3,4,5],[1,2,3,4,5])\n",
        "map_plot(quake_id=8288,predict=False,fig=fig,ax=ax1,interpolate=False,centered_map=False,n=300)\n",
        "#plt.scatter([1,2,3,4,5],[1,2,3,4,5])\n",
        "map_plot(quake_id=8288,predict=True,fig=fig,ax=ax2,interpolate=False,centered_map=False,n=300)"
      ],
      "metadata": {
        "id": "QBmEy9kxh6aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLbu3xbhV8_Q"
      },
      "source": [
        "# Stop there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCGvH5_DgCzj"
      },
      "outputs": [],
      "source": [
        " # print(data_eval,result_eval,sep='\\n')\n",
        "X = [x for x in range (0,len(data_eval))]\n",
        "# for x in X :\n",
        "#   print(data_eval[x:x+1],result_eval[x:x+1])\n",
        "model_predict_single = np.array([model.predict(data_eval[x:x+1]) for x in X])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjR_sMsJkAmc"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(figsize=(20,15))\n",
        "for x in X :\n",
        "  ax.plot(x,model_predict_single[x][0] ,\"k*\",markersize=10)\n",
        "  ax.plot(x,result_eval[x:x+1],\"b8\",markersize=10)\n",
        "plt.grid()\n",
        "plt.ylabel(\"PGA\",fontsize=40)\n",
        "plt.legend(['predicted values',\"real values\"],fontsize=25,markerscale=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuS2zvpMtCkO"
      },
      "outputs": [],
      "source": [
        "X1 = [x for x in range (0,len(data_train))]\n",
        "model_predict_single_train = np.array([model.predict(data_train[x:x+1]) for x in X1])\n",
        "fig1,ax1 = plt.subplots(figsize=(20,15))\n",
        "for x in X1 :\n",
        "  ax1.plot(x,model_predict_single_train[x][0] ,\"k*\",markersize=10)\n",
        "  ax1.plot(x,result_train[x:x+1],\"b8\",markersize=10)\n",
        "plt.grid()\n",
        "plt.ylabel(\"PGA\",fontsize=40)\n",
        "plt.legend(['predicted values',\"real values\"],fontsize=25,markerscale=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CIX2rl_RxqK"
      },
      "source": [
        "Idée à faire plus tard :\n",
        "- travailler sur des batch d'input au lieu de les traiter 1 par 1\n",
        "- Utiliser des modèle preentrainé"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1xvLkrkaYu7TSX4T4VEE8fItkKQM0WumA",
      "authorship_tag": "ABX9TyPdwPYdJ0plLn97R7cdsz43",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}